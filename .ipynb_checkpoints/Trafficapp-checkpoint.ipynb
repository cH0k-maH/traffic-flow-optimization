{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e90966-4c81-4089-92be-42f2fc2692cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load models\n",
    "vehicle_tracking_model = load_model('vehicle_tracking_model.h5')\n",
    "traffic_light_control_model = load_model('traffic_light_control_model.h5')\n",
    "congestion_detection_model = load_model('congestion_detection_model.h5')\n",
    "\n",
    "# Model options\n",
    "model_options = {\n",
    "    \"Vehicle Tracking\": vehicle_tracking_model,\n",
    "    \"Traffic Light Control\": traffic_light_control_model,\n",
    "    \"Congestion Detection\": congestion_detection_model\n",
    "}\n",
    "\n",
    "st.title(\"Traffic Management System\")\n",
    "\n",
    "# Select video input method\n",
    "video_input_option = st.radio(\"Select Video Input Method\", (\"Live Stream\", \"Upload Video\"))\n",
    "\n",
    "# Video upload\n",
    "uploaded_videos = []\n",
    "if video_input_option == \"Upload Video\":\n",
    "    uploaded_videos = st.file_uploader(\"Upload Video\", type=[\"mp4\", \"avi\"], accept_multiple_files=True, max_uploads=3)\n",
    "\n",
    "# Select models\n",
    "selected_models = st.multiselect(\"Select Models\", list(model_options.keys()))\n",
    "\n",
    "# Function to process video\n",
    "def process_video(video_path, models):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Resize frame for models (this example assumes a frame size of (128, 128))\n",
    "        resized_frame = cv2.resize(frame, (128, 128))\n",
    "        resized_frame = np.expand_dims(resized_frame, axis=0)\n",
    "\n",
    "        # Apply selected models\n",
    "        for model_name in models:\n",
    "            model = models[model_name]\n",
    "            prediction = model.predict(resized_frame)\n",
    "            st.write(f\"{model_name} prediction: {prediction}\")\n",
    "\n",
    "        # Display the frame in Streamlit\n",
    "        st.image(frame, channels=\"BGR\")\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "# Process live stream or uploaded videos\n",
    "if video_input_option == \"Live Stream\":\n",
    "    st.write(\"Live stream functionality will be implemented here.\")\n",
    "    # Implement live stream capture and processing\n",
    "\n",
    "elif video_input_option == \"Upload Video\":\n",
    "    for uploaded_video in uploaded_videos:\n",
    "        st.write(f\"Processing video: {uploaded_video.name}\")\n",
    "        video_path = uploaded_video.name\n",
    "        # Save uploaded file temporarily\n",
    "        with open(video_path, \"wb\") as f:\n",
    "            f.write(uploaded_video.getbuffer())\n",
    "        # Process video\n",
    "        process_video(video_path, model_options)\n",
    "        # Optionally, remove the temporary file after processing\n",
    "\n",
    "# Run the Streamlit app with:\n",
    "# streamlit run app.py\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Posture Recognition Env",
   "language": "python",
   "name": "posture_recognition_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
